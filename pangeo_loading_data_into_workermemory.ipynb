{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numpy import pi, sin, cos, arccos, clip, deg2rad\n",
    "import numpy.ma as ma\n",
    "from datetime import datetime\n",
    "import dask\n",
    "import time\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Currently 30 equally structured datasets stored in 30 zarr-stores.\n",
    "##### All dataset has the same length of tabular data,\n",
    "All 30 datasets has 5 columns,  `latitude`, `longitude`, `time`, `mean`, `anomalies`, each with shape = (2105319,), where mean and anomalies is the only value different between the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gcsfs\n",
    "\n",
    "with open('pangeo-181919-e7bc5bdaf4d5.json') as f:\n",
    "    token = json.load(f)\n",
    "gcs = gcsfs.GCSFileSystem(token=token)\n",
    "\n",
    "plevel = 4 # Choosing an arbitrary dataset out of the 30\n",
    "dspath='pangeo-argo-eke/global/mean_and_anomalies/with_pressure_coordinate/readable_ws_and_NHarm/mean_and_anomalies_global_ws500_plevel'+str(plevel)+'.zarr'\n",
    "mapper_alt0 = gcs.get_mapper(dspath)\n",
    "d = xr.open_zarr(mapper_alt0, consolidated=True, decode_cf=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fsspec.utils\n",
    "#fsspec.utils.setup_logging(logger_name=\"gcsfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7fe2c5f8b0d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean and anomalies for pressurelevel is saved to output-path:\n",
    "ds = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        Mean = ( [\"i\", \"pressure\"], d.Mean.load().values ),\n",
    "        Anomalies = ( [\"i\", \"pressure\"], d.Anomalies.load().values ),\n",
    "    ),\n",
    "    coords=dict(\n",
    "        time = ( [\"i\"], d.time.load().values ),\n",
    "        latitude = ( [\"i\"], d.latitude.load().values ),\n",
    "        longitude = ( [\"i\"], d.longitude.load().values ),\n",
    "        pressure = ([\"pressure\"], d.pressure.load().values ),\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description = 'Estimated mean dynamic height on profile-coordinates, and anomalies by subtracting estimated mean from observations',\n",
    "        pressureindex = d.pressureindex,\n",
    "        number_of_harmonics = d.number_of_harmonics,\n",
    "        window_size = d.window_size,\n",
    "        creation_date = str( datetime.now() )\n",
    "    ),\n",
    ")\n",
    "\n",
    "ds.time.attrs[\"standard_name\"] = 'time'\n",
    "ds.time.attrs[\"units\"] = 'days since 1970-01-01 00:00:00'\n",
    "ds.latitude.attrs[\"standard_name\"] = 'latitude'\n",
    "ds.latitude.attrs[\"units\"] = 'degrees_north'\n",
    "ds.longitude.attrs[\"standard_name\"] = 'longitude'\n",
    "ds.longitude.attrs[\"units\"] = 'degrees_east'\n",
    "ds.pressure.attrs[\"standard_name\"] = 'pressure'\n",
    "ds.pressure.attrs[\"units\"] = 'decibar'\n",
    "ds.Mean.attrs[\"standard_name\"] = 'Estimated mean dynamic height'\n",
    "ds.Mean.attrs[\"units\"] = 'm^2/s^2'\n",
    "ds.Anomalies.attrs[\"standard_name\"] = 'dynamic height anomalies'\n",
    "ds.Anomalies.attrs[\"units\"] = 'm^2/s^2'\n",
    "\n",
    "dsc = ds.chunk()\n",
    "outfile = 'pangeo-argo-eke/chunk_alternatives/global_mean_and_anomalies_plevel'+str(plevel)+'_current_zarr-storing-code.zarr'\n",
    "mapper_alt1 = gcs.get_mapper(outfile)\n",
    "dsc.to_zarr(mapper_alt1, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7fe2c5f8b290>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean and anomalies for pressurelevel is saved to output-path:\n",
    "ds = xr.Dataset(\n",
    "    data_vars = dict(\n",
    "        Mean = ( [\"i\", \"pressure\"], d.Mean.load().values ),\n",
    "        Anomalies = ( [\"i\", \"pressure\"], d.Anomalies.load().values ),\n",
    "    ),\n",
    "    coords=dict(\n",
    "        time = ( [\"i\"], d.time.load().values ),\n",
    "        latitude = ( [\"i\"], d.latitude.load().values ),\n",
    "        longitude = ( [\"i\"], d.longitude.load().values ),\n",
    "        pressure = ([\"pressure\"], d.pressure.load().values ),\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description = 'Estimated mean dynamic height on profile-coordinates, and anomalies by subtracting estimated mean from observations',\n",
    "        pressureindex = d.pressureindex,\n",
    "        number_of_harmonics = d.number_of_harmonics,\n",
    "        window_size = d.window_size,\n",
    "        creation_date = str( datetime.now() )\n",
    "    ),\n",
    ")\n",
    "\n",
    "ds.time.attrs[\"standard_name\"] = 'time'\n",
    "#ds.time.attrs[\"units\"] = 'days since 1970-01-01 00:00:00'\n",
    "ds.latitude.attrs[\"standard_name\"] = 'latitude'\n",
    "ds.latitude.attrs[\"units\"] = 'degrees_north'\n",
    "ds.longitude.attrs[\"standard_name\"] = 'longitude'\n",
    "ds.longitude.attrs[\"units\"] = 'degrees_east'\n",
    "ds.pressure.attrs[\"standard_name\"] = 'pressure'\n",
    "ds.pressure.attrs[\"units\"] = 'decibar'\n",
    "ds.Mean.attrs[\"standard_name\"] = 'Estimated mean dynamic height'\n",
    "ds.Mean.attrs[\"units\"] = 'm^2/s^2'\n",
    "ds.Anomalies.attrs[\"standard_name\"] = 'dynamic height anomalies'\n",
    "ds.Anomalies.attrs[\"units\"] = 'm^2/s^2'\n",
    "\n",
    "dsc = ds.chunk()\n",
    "outfile = 'pangeo-argo-eke/chunk_alternatives/global_mean_and_anomalies_plevel'+str(plevel)+'_time-dtype-float64.zarr'\n",
    "mapper_alt2 = gcs.get_mapper(outfile)\n",
    "dsc.to_zarr(mapper_alt2, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of each dataset in megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.212768"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsc.nbytes/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to a dask-cluster and set cluster-options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_gateway import GatewayCluster, Gateway\n",
    "from distributed import Client\n",
    "\n",
    "g = Gateway()\n",
    "g.list_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = g.connect(g.list_clusters()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = g.cluster_options()\n",
    "options.worker_cores = 2; options.worker_memory = 4\n",
    "# Create a cluster with those options\n",
    "cluster = g.new_cluster(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.list_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-a8d422c9-2c8a-11ee-83c6-decc245b3c5b</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_gateway.GatewayCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/services/dask-gateway/clusters/prod.3d80ce07c5574c1881a4e47e530d5240/status\" target=\"_blank\">/services/dask-gateway/clusters/prod.3d80ce07c5574c1881a4e47e530d5240/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/services/dask-gateway/clusters/prod.3d80ce07c5574c1881a4e47e530d5240/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>GatewayCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Name: </b>prod.3d80ce07c5574c1881a4e47e530d5240\n",
       "    <li><b>Dashboard: </b><a href='/services/dask-gateway/clusters/prod.3d80ce07c5574c1881a4e47e530d5240/status' target='_blank'>/services/dask-gateway/clusters/prod.3d80ce07c5574c1881a4e47e530d5240/status</a>\n",
       "  </ul>\n",
       "</div>\n",
       "\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tls://10.8.0.6:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading of data from cloud-storage<br>\n",
    "Want to load 4 columns of data, `latitude`, `longitude`, `time`, `anomalies`, into memory and do a computation. Which is done embarrasingly parallel ( x 1e6  )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_chunksauto(mapper):\n",
    "    \"\"\"Load data from zarr-store into worker-memory\"\"\"\n",
    "    \n",
    "    ds = xr.open_zarr(mapper, consolidated=True, chunks='auto') \n",
    "    data0 = ds.Anomalies.sel(pressure=50).load().values\n",
    "    ii = ~xr.apply_ufunc(np.isnan, data0)\n",
    "    data, lat, lon, time = data0[ii], ds.latitude.load().values[ii], ds.longitude.load().values[ii], ds.time.load().values[ii]\n",
    "    \n",
    "    # Calculation using data-, lat-, lon-, and time-arrays\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_chunksNone(mapper):\n",
    "    \"\"\"Load data from zarr-store into worker-memory\"\"\"\n",
    "    \n",
    "    ds = xr.open_zarr(mapper, consolidated=True, chunks=None) \n",
    "    data0 = ds.Anomalies.sel(pressure=50).values\n",
    "    ii = ~xr.apply_ufunc(np.isnan, data0)\n",
    "    data, lat, lon, time = data0[ii], ds.latitude.values[ii], ds.longitude.values[ii], ds.time.values[ii]\n",
    "    \n",
    "    # Calculation using data-, lat-, lon-, and time-arrays\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def zarr_open(mapper):\n",
    "    \"\"\"Load data from zarr-store into worker-memory\"\"\"\n",
    "    \n",
    "    z = zarr.open_consolidated(mapper)\n",
    "    data0 = np.squeeze(z.Anomalies[:])\n",
    "    ii = ~np.isnan(data0)\n",
    "    data, lat, lon, time = data0[ii], z.latitude[:][ii], z.longitude[:][ii], z.time[:][ii]\n",
    "    \n",
    "    # Calculation using data-, lat-, lon-, and time-arrays\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def zarr_convenience_open(mapper):\n",
    "    \"\"\"Load data from zarr-store into worker-memory\"\"\"\n",
    "    \n",
    "    z = zarr.convenience.open_consolidated(mapper)\n",
    "    data0 = np.squeeze(z.Anomalies[:])\n",
    "    ii = ~np.isnan(data0)\n",
    "    data, lat, lon, time = data0[ii], z.latitude[:][ii], z.longitude[:][ii], z.time[:][ii]\n",
    "    \n",
    "    # Calculation using data-, lat-, lon-, and time-arrays\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timing loading of data:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "ds = xr.open_zarr(mapper, consolidated=True, chunks='auto'); ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.47 s ± 92.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( load_chunksauto(mapper_alt1) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 s ± 90.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( load_chunksauto(mapper_alt2) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "ds = xr.open_zarr(mapper, consolidated=True, chunks=None); ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.61 s ± 84.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( load_chunksNone(mapper_alt1) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 s ± 107 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( load_chunksNone(mapper_alt2) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "z = zarr.open_consolidated(mapper); ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05 s ± 34.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( zarr_open(mapper_alt1) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 s ± 106 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( zarr_open(mapper_alt2) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "z = zarr.convenience.open_consolidated(mapper); ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05 s ± 31.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( zarr_convenience_open(mapper_alt1) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37 s ± 178 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dask.compute( zarr_convenience_open(mapper_alt2) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling down and closing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 15:17:46,375 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
